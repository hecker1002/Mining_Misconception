{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":27644,"sourceType":"modelInstanceVersion","modelInstanceId":23286,"modelId":33601},{"sourceType":"datasetVersion","sourceId":6402147,"datasetId":3691353}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport torch ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-05T14:32:21.808313Z","iopub.execute_input":"2024-10-05T14:32:21.808787Z","iopub.status.idle":"2024-10-05T14:32:25.229738Z","shell.execute_reply.started":"2024-10-05T14:32:21.808736Z","shell.execute_reply":"2024-10-05T14:32:25.228902Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\nmisconception = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv')\ntest_data  = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.231205Z","iopub.execute_input":"2024-10-05T14:32:25.231627Z","iopub.status.idle":"2024-10-05T14:32:25.289150Z","shell.execute_reply.started":"2024-10-05T14:32:25.231593Z","shell.execute_reply":"2024-10-05T14:32:25.288354Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"''' \n\nSO trian data - has a question(simple maths )\nand answers are - A,B ,C ,D out of which ONly 1 is correct . \n\nNow , if opttion D is correct , other option (A ,B,C) wrong ,  \nSo we are also given reaseon for them being wrong( using misconception id no. from \nmisconception dataframe ). \n\nnow ,we need to predict this misconception id for test data . \n\ne.g. Do 2 x3 - 5 . \n\nA. 1 B. 0 C. 9 D. 2 \ncorrect ans =A \nwrong ans = B ,C ,D \n\nmsconceptionA - nan , misconcB  -23 id no (wronf rules) , misconcepC (wrong info) \n\nPredict misconception wrt a question \n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.290222Z","iopub.execute_input":"2024-10-05T14:32:25.290535Z","iopub.status.idle":"2024-10-05T14:32:25.297942Z","shell.execute_reply.started":"2024-10-05T14:32:25.290502Z","shell.execute_reply":"2024-10-05T14:32:25.297099Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"' \\n\\nSO trian data - has a question(simple maths )\\nand answers are - A,B ,C ,D out of which ONly 1 is correct . \\n\\nNow , if opttion D is correct , other option (A ,B,C) wrong ,  \\nSo we are also given reaseon for them being wrong( using misconception id no. from \\nmisconception dataframe ). \\n\\nnow ,we need to predict this misconception id for test data . \\n\\ne.g. Do 2 x3 - 5 . \\n\\nA. 1 B. 0 C. 9 D. 2 \\ncorrect ans =A \\nwrong ans = B ,C ,D \\n\\nmsconceptionA - nan , misconcB  -23 id no (wronf rules) , misconcepC (wrong info) \\n\\nPredict misconception wrt a question \\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"train_data.iloc[1]","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.300718Z","iopub.execute_input":"2024-10-05T14:32:25.301048Z","iopub.status.idle":"2024-10-05T14:32:25.315120Z","shell.execute_reply.started":"2024-10-05T14:32:25.301013Z","shell.execute_reply":"2024-10-05T14:32:25.314239Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"QuestionId                                                          1\nConstructId                                                      1612\nConstructName       Simplify an algebraic fraction by factorising ...\nSubjectId                                                        1077\nSubjectName                           Simplifying Algebraic Fractions\nCorrectAnswer                                                       D\nQuestionText        Simplify the following, if possible: \\( \\frac{...\nAnswerAText                                                 \\( m+1 \\)\nAnswerBText                                                 \\( m+2 \\)\nAnswerCText                                                 \\( m-1 \\)\nAnswerDText                                         Does not simplify\nMisconceptionAId                                               2142.0\nMisconceptionBId                                                143.0\nMisconceptionCId                                               2142.0\nMisconceptionDId                                                  NaN\nName: 1, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"index =1 \n\n\n\ndef dataset_load( index ) :\n    \n\n    idea = train_data.iloc[index]['ConstructName']\n    quesn = train_data.iloc[index]['QuestionText']\n    ans_opt = train_data.iloc[index]['CorrectAnswer']\n    ans = ('Answer' + ans_opt + 'Text')\n    ans = train_data.iloc[index][ans]\n\n    prompt1  = f'The basic idea of this question is to {idea}.Now ,the actual question is to - {quesn}.'\n\n    prompt2 =f'now the correct answer is- Option{ans_opt},that is {ans}.'\n\n    prompt3  = f'The rest of the answers are wrong. And the reason for this misconception are-'\n\n\n    options = ['A' ,'B' ,'C' ,'D'] \n\n    quesn_prompt = prompt1\n    ans_prompt =  prompt2 + prompt3 \n\n    full_ans_str = ''\n    \n    Misc_ID = []\n    \n    for opt in options :\n        if opt is not np.nan :\n            misc = ('Misconception'+ opt+ 'Id')\n            misc_value = train_data.iloc[index][misc]\n            \n            # Check if the misconception ID is not NaN\n            if pd.notna(misc_value):\n                misc_id = int(misc_value)\n                misc_text = misconception.iloc[misc_id][1]\n\n                prompt_ = f'Option {opt} is wrong because {misc_text}.'\n                ans_prompt += prompt_ + f'hence , the misconception id are-{misc_id}\\n'\n                full_ans_str+=ans_prompt\n                Misc_ID.append( str(misc_id) )\n\n                \n    quesn_str = '<s>[INST]' + quesn_prompt + '[/INST]'\n    misc_ = ' '.join(Misc_ID)\n    full_ans_str_ = f'The Misconception ID are {misc_}\\n'\n    final_ans = full_ans_str_ + full_ans_str\n    \n    Question.append(quesn_str)\n    Misconception_Id.append( final_ans )\n           \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.316283Z","iopub.execute_input":"2024-10-05T14:32:25.316603Z","iopub.status.idle":"2024-10-05T14:32:25.326357Z","shell.execute_reply.started":"2024-10-05T14:32:25.316571Z","shell.execute_reply":"2024-10-05T14:32:25.325356Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"Question = []\nMisconception_Id = []\n\n\nfor index in range( 50 ) :\n    dataset_load(index)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.327462Z","iopub.execute_input":"2024-10-05T14:32:25.327779Z","iopub.status.idle":"2024-10-05T14:32:25.377919Z","shell.execute_reply.started":"2024-10-05T14:32:25.327745Z","shell.execute_reply":"2024-10-05T14:32:25.377070Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/4197465484.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  misc_text = misconception.iloc[misc_id][1]\n","output_type":"stream"}]},{"cell_type":"code","source":"data = { 'Question' :Question , 'Misconception_Id': Misconception_Id }\n\ndf = pd.DataFrame( data)\n\ndf.iloc[1][0]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.378869Z","iopub.execute_input":"2024-10-05T14:32:25.379134Z","iopub.status.idle":"2024-10-05T14:32:25.386321Z","shell.execute_reply.started":"2024-10-05T14:32:25.379103Z","shell.execute_reply":"2024-10-05T14:32:25.385398Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1465143998.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  df.iloc[1][0]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'<s>[INST]The basic idea of this question is to Simplify an algebraic fraction by factorising the numerator.Now ,the actual question is to - Simplify the following, if possible: \\\\( \\\\frac{m^{2}+2 m-3}{m-3} \\\\).[/INST]'"},"metadata":{}}]},{"cell_type":"code","source":"''' Think how to prepare datset '''\nText = []\n\nfor i in range( df.shape[0]) :\n    seg = df.iloc[i][0] + df.iloc[i][1]\n    Text.append( seg )\n\ndata ={ 'text' : Text }\n\ndata1 = pd.DataFrame( data)\ndata1.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.387414Z","iopub.execute_input":"2024-10-05T14:32:25.387722Z","iopub.status.idle":"2024-10-05T14:32:25.407947Z","shell.execute_reply.started":"2024-10-05T14:32:25.387691Z","shell.execute_reply":"2024-10-05T14:32:25.407096Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3545490196.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  seg = df.iloc[i][0] + df.iloc[i][1]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                text\n0  <s>[INST]The basic idea of this question is to...\n1  <s>[INST]The basic idea of this question is to...\n2  <s>[INST]The basic idea of this question is to...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;s&gt;[INST]The basic idea of this question is to...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;s&gt;[INST]The basic idea of this question is to...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;s&gt;[INST]The basic idea of this question is to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# !pip install datasets \n\nfrom datasets import Dataset, DatasetDict\n\ndata_new  = Dataset.from_pandas(data1)\n#data_new.push_to_hub(\"misconception-llama2-1k\")\n# data_new.save_to_disk(\"data_new.hf\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:25.409106Z","iopub.execute_input":"2024-10-05T14:32:25.409386Z","iopub.status.idle":"2024-10-05T14:32:26.266754Z","shell.execute_reply.started":"2024-10-05T14:32:25.409355Z","shell.execute_reply":"2024-10-05T14:32:26.265860Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# !pip install -q accelerate peft bitsandbytes transformers trl\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:26.269694Z","iopub.execute_input":"2024-10-05T14:32:26.270107Z","iopub.status.idle":"2024-10-05T14:32:26.274293Z","shell.execute_reply.started":"2024-10-05T14:32:26.270073Z","shell.execute_reply":"2024-10-05T14:32:26.273417Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Importing important Libraries \n# peft = Paramter efficient fine tuning of Large language models (contans Lora method ) \n# to save GPU\n# bitsandbytes for Quantization \n\n!pip install -q accelerate peft bitsandbytes transformers trl\n\n# import other transformer imp. modules \nimport os \nimport torch \n\ntorch.cuda.empty_cache()\n\n# dAtaset \nfrom datasets import load_dataset \n\nfrom transformers import (\n    AutoModelForCausalLM ,\n    AutoTokenizer ,\n    BitsAndBytesConfig , \n    HfArgumentParser , \n    TrainingArguments , \n    pipeline , \n    logging \n)\n\n# Lora and Transfre learning approaches downloaded \nfrom peft import LoraConfig , PeftModel \nfrom trl import SFTTrainer \n# Peft - works liek Transfer learning . \n\n# while fine tuning a pre-trained model(GPT) on our speciic daatset , freezes some \n# weights and biases(that carry imp info) and trians on specific dataset. \n\n# laod the model \nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\n\n\n# custom datset (Q-A dataset)\n# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n\n# Prepare before hand new model name  \nfine_tune_model_name = \"LLama-7b-fine-tuned \"\n\n# QLora (hypermeters)\n\n# dim of the low rank matrixes that will store the chnages in pre-trained weights . \nlora_rank = 64 \n\n# for scaling in QLora (quantization )\nlora_alpha = 16\n\n\n# to prevent ovefitting , dropout \nlora_dropout = 0.1  \n\n# Quantization Pramas \n\n# to store weights in int( 4 bits)\nuse_4bit = True \n\n# orginal weight data type \ndtype_4bit= \"float16\"\n\nquant_dtype_4bit = \"nf4\" # either nf4 or fp4 \n\n# nested quantization \nuse_nested_quant  = False \n\n\noutput_dir = \"./results\"\n# trianing( fien tuning params )\n\nnum_epochs =1\n\nfp_16 = False \nbp_16 =False \n\n# batch size per gpuu for trianing \nbatch_size_per_gpu_train = 4 \n\n# batch size per gpuu for eval\nbatch_size_per_gpu_eval = 4 \n\ngradient_accum_step =1 \n\n#check gradients of loss \ngradient_checkpointing = True \n\n#maximum grad noemal \nmax_grad_norm = 0.3  \n\nlearning_rate = 0.1 \n\n# for alyer norm \nweight_decay = 0.001 \n\n#optimizer \noptimizer = \"paged_adamw_32bit\"\n\n# cosie simi learning \nlr_scheduler_type = \"cosine\"\n\n#override steps \nmax_steps =-1 \n\n#warmpup ratio (re trian frm same point)\nwarm_up = 0.03 \n\n\n# now to speed up training \ngroup_by_length = True \n\nsave_steps= 0 \n\nlogging_steps= 10\n\n# Supervised tuning (SFT )\n\nmax_seq_length = None \npacking =False \n\n\n# laod the doel on gpu 0\ndevice_map = {\"\": 0 }\n\n\n# Load dataset \n# dataset = load_dataset ( dataset_name  ,split  =\"train\")\ndataset = data_new \n\n# dataset = dataset.train_test_split(test_size=0.9)['train']\n\n# LAOD MODEL AND TOKENIZER WITHQLORA HYPER\n\ncompute_dtype = getattr( torch , dtype_4bit )\n\n#Quantized model loading \nbnb_config  = BitsAndBytesConfig(\n    load_in_4bit = use_4bit , \n    \n    bnb_4bit_quant_type = quant_dtype_4bit  , \n    \n    bnb_4bit_compute_dtype = dtype_4bit  , \n    \n    bnb_4it_sue_doublequant = use_nested_quant \n)\n\n# Laod the base model(Llama) - pre-trianed  trained on books web , to usderstannf language\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name , \n    quantization_config = bnb_config, \n    device_map = device_map ,\n)\n\nmodel.config.use_cache = False \nmodel.config.pretraining_tp = 1 \n\n''' RUns ONLY on GPU (this code )'''\n# Load tokenzier  \n# to convert word into word embeddings  \n\n#llama tokeizer \ntokenizer = AutoTokenizer.from_pretrained( model_name , trust_remote_code = True )\ntokenizer.pad_token = tokenizer.eos_token # end of speed  token \ntokenizer.padding_size = 'right'\n\n# Laod Lora Config \n\npeft_config = LoraConfig (\n    lora_alpha = lora_alpha , \n    lora_dropout = lora_dropout , \n    r = lora_rank , \n    bias =\"none\" ,\n    task_type= \"CAUSAL_LM\"\n)\n# Define trainign arguments of LLama 2 model for fien-tuning \n\ntraining_arguments  = TrainingArguments (\n    \n    output_dir= \"./results\" , # output folder \n    \n    num_train_epochs =  2  , # epochs for trianig \n    \n    per_device_train_batch_size = 2 , \n    \n    gradient_accumulation_steps  = 2 , \n    \n    optim = \"paged_adamw_32bit\" , \n    \n    save_steps = 2 , \n    \n    logging_steps = 10 , \n    \n    learning_rate = 2e-3 , # keep it low for better converge in backprop \n    \n    weight_decay = 0.001 , \n    \n    fp16  = True  , \n    bf16 =  False , \n    \n    max_grad_norm = 0.3 , \n    \n    max_steps = -1 , \n    \n    warmup_ratio = 0.03  , \n    \n    group_by_length = True , \n    \n    lr_scheduler_type = \"cosine\", \n    \n    report_to = \"tensorboard\"\n    \n)\n# Set SFT trainign arguments \n\ntrainer = SFTTrainer (\n    \n    model = model , \n    train_dataset = dataset , \n    peft_config = peft_config , \n    \n    dataset_text_field = \"text\" , \n    \n    max_seq_length = max_seq_length , \n    \n    tokenizer = tokenizer , \n    \n    args = training_arguments , \n    \n    packing = packing \n)\n\n# # export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n# nvidia-smi\n\nimport os\n\n# Set the environment variable\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:32:26.275874Z","iopub.execute_input":"2024-10-05T14:32:26.276212Z","iopub.status.idle":"2024-10-05T14:34:08.000330Z","shell.execute_reply.started":"2024-10-05T14:32:26.276180Z","shell.execute_reply":"2024-10-05T14:34:07.999527Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Unused kwargs: ['bnb_4it_sue_doublequant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0f66bf24934a9cbca748f1c3947228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b46b26ccd944579a3987bca095bf5c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313cbbf8b9d64fcdba9344f74fa3dfca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650357b9800441bba8912e6f0d90629f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2117860163b249eebf7a287989f3a0a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f100f54c6aaf4c41ab4612e07da8c491"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1f0ed01c824a66973108efac25f941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65e46bd2e524686bff5894ae62f0cae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c03446b1ea48feaeaf47812e01e30c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4ba8e841d846c5b77199c6e383735d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9d872e3df8418da1e238bb68bcbc40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05aa35957b664d99937450e691bd7b0b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257584dba54343878acb962b2432e760"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:34:08.001594Z","iopub.execute_input":"2024-10-05T14:34:08.002191Z","iopub.status.idle":"2024-10-05T14:37:17.609763Z","shell.execute_reply.started":"2024-10-05T14:34:08.002154Z","shell.execute_reply":"2024-10-05T14:37:17.608868Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [24/24 02:58, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.503500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.664900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=24, training_loss=0.9985543588797251, metrics={'train_runtime': 188.5899, 'train_samples_per_second': 0.53, 'train_steps_per_second': 0.127, 'total_flos': 1415866027425792.0, 'train_loss': 0.9985543588797251, 'epoch': 1.92})"},"metadata":{}}]},{"cell_type":"code","source":"i =1 \nprompt = 'Solve this question ' + test_data.iloc[i]['QuestionText'] + \" where \"\n    \nprompt += 'Option A is ' + test_data.iloc[i]['AnswerAText'] + \" and \\n\"\nprompt += 'Option B is ' + test_data.iloc[i]['AnswerBText'] + \" and \\n\"\nprompt += 'Option C is ' + test_data.iloc[i]['AnswerCText'] + \" and \\n\"\nprompt += 'Option D is ' + test_data.iloc[i]['AnswerDText'] + \" and \\n\"\n    \nprompt += \"Give in Only 1 letter the correct Option like -A or B or C or D.\"\n    \n# Generate the option response\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\nopt = pipe(f\"<s>[INST]{prompt}[/INST]\")[0]['generated_text']\n    \nanswer_opt = opt.replace(prompt, \"\").strip()\n# answer_opt = answer_opt[:10]\nprint(answer_opt)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:37:17.610910Z","iopub.execute_input":"2024-10-05T14:37:17.611599Z","iopub.status.idle":"2024-10-05T14:37:47.642462Z","shell.execute_reply.started":"2024-10-05T14:37:17.611562Z","shell.execute_reply":"2024-10-05T14:37:47.641546Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST][/INST]The Misconception ID are 2212 1756 1756 1756\nnow the correct answer is- Option A,that is \\( m+1 \\).The rest of the answers are wrong. And the reason for this misconception are-Option B is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-1756\nnow the correct answer is- Option A,that is \\( m+1 \\).The rest of the answers are wrong. And the reason for this misconception are-Option B is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-1756\nOption C is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-1756\nnow the correct answer is- Option A,that is \\( m+1 \\).The rest of the answers are wrong. And the reason for this misconception are-Option B is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-1756\nOption C is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-1756\nOption D is wrong because When simplifying fractions, divides both terms by a common denominator.hence, the misconception id are-2212\nnow the correct answer is- Option A,that is \\( m+1 \\).The rest of the answers are wrong. And the reason for this misconception are-Option B is wrong because When simplifying fractions, divides both terms\n","output_type":"stream"}]},{"cell_type":"code","source":"import re \n\n# 1. Extract misconception IDs (numbers after 'misconception id are-')\nmisconception_ids = re.findall(r'misconception id are-(\\d+)', answer_opt)\n\n# 2. Extract correct option (letter after 'correct answer is- Option')\ncorrect_option = re.search(r'correct answer is- Option ([A-D])', answer_opt).group(1)\n\n# 3. Join misconception IDs into a single string separated by spaces\nmisconception_ids_str = ' '.join(misconception_ids)\n\n# Output the results\nprint(f\"Misconception IDs: {misconception_ids_str}\")\nprint(f\"Correct Option: {correct_option}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:37:47.643629Z","iopub.execute_input":"2024-10-05T14:37:47.643930Z","iopub.status.idle":"2024-10-05T14:37:47.650507Z","shell.execute_reply.started":"2024-10-05T14:37:47.643896Z","shell.execute_reply":"2024-10-05T14:37:47.649635Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Misconception IDs: 1756 1756 1756 1756 1756 2212\nCorrect Option: A\n","output_type":"stream"}]},{"cell_type":"code","source":"Question_id = []\nmisc_id = []\n\nimport re \n\nfor i in range( test_data.shape[0] ):\n    # Construct the question prompt\n    prompt = 'Solve this question ' + test_data.iloc[i]['QuestionText'] + \" where \"\n    \n    prompt += 'Option A is ' + test_data.iloc[i]['AnswerAText'] + \" and \\n\"\n    prompt += 'Option B is ' + test_data.iloc[i]['AnswerBText'] + \" and \\n\"\n    prompt += 'Option C is ' + test_data.iloc[i]['AnswerCText'] + \" and \\n\"\n    prompt += 'Option D is ' + test_data.iloc[i]['AnswerDText'] + \" and \\n\"\n    \n    prompt += \"Give in Only 1 letter the correct Option like -A or B or C or D.\"\n    \n    # Generate the option response\n    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n    opt = pipe(f\"<s>[INST]{prompt}[/INST]\")[0]['generated_text']\n    \n    answer_opt = opt.replace(prompt, \"\").strip()\n    \n    misconception_ids = re.findall(r'misconception id are-(\\d+)', answer_opt)\n\n# 2. Extract correct option (letter after 'correct answer is- Option')\n    correct_option = re.search(r'correct answer is- Option ([A-D])', answer_opt).group(1)\n\n# 3. Join misconception IDs into a single string separated by spaces\n    misconception_ids_str = ' '.join(misconception_ids)\n\n\n    # Convert QuestionId to string before concatenation\n    Question_id.append(str(test_data.iloc[i]['QuestionId']) + '_' + correct_option)\n    misc_id.append( misconception_ids_str )\n    \n    # Misconception ID prompt\n#     prompt_ = 'Give Only the Misconception Id for this question ' + test_data.iloc[i]['QuestionText'] + \" with respect to \"\n    \n#     misc_ID = []\n#     for option in ['A', 'B', 'C', 'D']:\n#         # Access the correct AnswerText based on the option\n#         if option == 'A':\n#             answer_text = test_data.iloc[i]['AnswerAText']\n#         elif option == 'B':\n#             answer_text = test_data.iloc[i]['AnswerBText']\n#         elif option == 'C':\n#             answer_text = test_data.iloc[i]['AnswerCText']\n#         elif option == 'D':\n#             answer_text = test_data.iloc[i]['AnswerDText']\n        \n#         prompt1 = 'Option ' + option + ' is ' + answer_text + \". \"\n#         prompt_ += prompt1 + \"Give Only the number corresponding to the misconception Id. No text should be given in output, only number separated with spaces.\"\n        \n#         # Generate the misconception ID response\n#         pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length= 500)\n#         _id = pipe(f\"<s>[INST]{prompt_}[/INST]\")[0]['generated_text']\n#         answer_id = _id.replace(prompt_, \"\").strip()\n#         misc_ID.append(str(answer_id))\n    \n    # Join the misconception IDs with spaces\n   \n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:37:47.651769Z","iopub.execute_input":"2024-10-05T14:37:47.652089Z","iopub.status.idle":"2024-10-05T14:39:06.958282Z","shell.execute_reply.started":"2024-10-05T14:37:47.652058Z","shell.execute_reply":"2024-10-05T14:39:06.957407Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"ans = { 'QuestionId_Answer' : Question_id , \"MisconceptionId\" : misc_id}\nans_df = pd.DataFrame( ans )\nans_df.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:39:06.959807Z","iopub.execute_input":"2024-10-05T14:39:06.960224Z","iopub.status.idle":"2024-10-05T14:39:06.971990Z","shell.execute_reply.started":"2024-10-05T14:39:06.960167Z","shell.execute_reply":"2024-10-05T14:39:06.971035Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"  QuestionId_Answer                MisconceptionId\n0            1869_B            2480 2480 1888 2480\n1            1870_B  2162 2162 1122 2162 1122 1122\n2            1871_B            1518 1518 1518 1518","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QuestionId_Answer</th>\n      <th>MisconceptionId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1869_B</td>\n      <td>2480 2480 1888 2480</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1870_B</td>\n      <td>2162 2162 1122 2162 1122 1122</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1871_B</td>\n      <td>1518 1518 1518 1518</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"ans_df.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created successfully!\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:39:06.973107Z","iopub.execute_input":"2024-10-05T14:39:06.973416Z","iopub.status.idle":"2024-10-05T14:39:06.992617Z","shell.execute_reply.started":"2024-10-05T14:39:06.973382Z","shell.execute_reply":"2024-10-05T14:39:06.991724Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Submission file created successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}